{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojPrlhkmv-L0"
      },
      "source": [
        "# Week 3 Assignment:  Practicing with NumPy + Matplolib via basic analysis and visualization of ARGO data \n",
        "### What is ARGO data?\n",
        "- vertical profiles of pressure, temperature, salinity, etc. collected by a fleet of robotic instruments\n",
        "- drift with the ocean currents, sinks and rises\n",
        "- spend most of time under the surface! \n",
        "- the instruments collect extremely valuable data that helps us:\n",
        "  - better estimate heat storage in the ocean (and thus sea level rise predictions)\n",
        "  - better evaluate changes in salinity (and tie these to changes in global rainfall)\n",
        "\n",
        "### Where is ARGO data collected?\n",
        "![image.png](attachment:image.png)  \n",
        "(Can't see this image?  Visit: https://fleetmonitoring.euro-argo.eu/dashboard )\n",
        "  \n",
        "### Where can we access ARGO data?\n",
        "- nice outline of options for data access: https://argo.ucsd.edu/data/\n",
        "    - Example: http://www.argodatamgt.org/\n",
        "\n",
        "\n",
        "### Want more information on ARGO data?\n",
        "- https://argo.ucsd.edu/about/\n",
        "- https://youtu.be/PzHZdwaBr_Q (neat video about the journey of an ARGO float!)\n",
        "\n",
        "\n",
        "### How are we using ARGO data in this assignment?\n",
        "- data from a single \"float\" (instrument), collected between 2012-2014, as it journeys through the ocean - drifting and sinking and rising!\n",
        "<br><br>\n",
        "- **variables**: \n",
        "  - temperature (in degrees C)\n",
        "  - salinity (saltiness; in psu)\n",
        "  - pressure (weight of ocean above; in dbar)\n",
        "  - latitude (deg N or S)\n",
        "  - longitude (deg E or W)\n",
        "  - date (remember that the float drifts with the ocean currents.... so each vertical profile of temperature, salinity, and pressure is associated with a given date/time and a given location - i.e., the latitude/longitude point) \n",
        "<br><br>\n",
        "- **format**:  special NumPy array format - since you haven't learned much yet about reading in and working with data from Netcdf files (which is native format of current ARGO data)\n",
        "  - note: when you read in the data from the file, it is a data object that contains multiple arrays\n",
        "     - you'll find soon that this is analogous to data objects in Xarray (package we use for spatial data, especially that of Netcdf format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvPpuFqv-L3"
      },
      "source": [
        "#### (a.)\n",
        "- import the NumPy package\n",
        "- import the pyplot package from the Matplotlib package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dkvHstoLv-L3"
      },
      "outputs": [],
      "source": [
        "# import the NumPy package\n",
        "import numpy as np\n",
        "# import the pyplot package from the Matplotlib package\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MQYEp1Yv-L4"
      },
      "source": [
        "#### (b.)\n",
        "- download the ARGO data from the Week 3 Assignment folder, 'argo_float_4901412.npz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "40ocsIBfv-L4"
      },
      "outputs": [],
      "source": [
        "# Downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOwupXuv-L4"
      },
      "source": [
        "#### (c.)\n",
        "- load the data file (I'll do this for you!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOQswvQRv-L4",
        "outputId": "f3dc638a-6958-40e6-eb96-95cf404b1f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['S', 'T', 'levels', 'lon', 'date', 'P', 'lat']\n"
          ]
        }
      ],
      "source": [
        "# np.load loads in data from a special NumPy array file format\n",
        "data = np.load('argo_float_4901412.npz')\n",
        "\n",
        "# These are the arrays contained in this data object\n",
        "# Salinity, temperature, vertical levels in the ocean, longitude, date/time, pressure, and latitude in the order they appear \n",
        "print(data.files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw0ZL1Qwv-L5"
      },
      "source": [
        "#### (d.)\n",
        "- extract the arrays shown in (c.) to NumPy array variables of the same name\n",
        "  - *hint*: T = data['T'] to access each array within the 'data' data object - like a key for a dictionary\n",
        "- print out the values of the latitude, longitude, and date variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q7ghvl0Jv-L6"
      },
      "outputs": [],
      "source": [
        "# extract the arrays shown in (c.) to NumPy array variables of the same name\n",
        "# hint: T = data['T'] to access each array within the 'data' data object - like a key for a dictionary\n",
        "S = data['S']\n",
        "T = data['T']\n",
        "levels = data['levels']\n",
        "lon = data['lon']\n",
        "date = data['date']\n",
        "P = data['P']\n",
        "lat = data['lat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD036J4Rv-L6",
        "outputId": "7d237903-5125-4dd8-becd-e15d627468c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "latitude: [47.187 46.716 46.45  46.23  45.459 44.833 44.452 44.839 44.956 44.676\n",
            " 44.13  43.644 43.067 42.662 42.513 42.454 42.396 42.256 42.089 41.944\n",
            " 41.712 41.571 41.596 41.581 41.351 41.032 40.912 40.792 40.495 40.383\n",
            " 40.478 40.672 41.032 40.864 40.651 40.425 40.228 40.197 40.483 40.311\n",
            " 40.457 40.463 40.164 40.047 39.963 40.122 40.57  40.476 40.527 40.589\n",
            " 40.749 40.993 41.162 41.237 41.448 41.65  42.053 42.311 42.096 41.683\n",
            " 41.661 41.676 42.018 42.395 42.532 42.558 42.504 42.63  42.934 42.952\n",
            " 42.777 42.722 42.601 42.457 42.379]\n",
            "longitude: [-39.13  -37.282 -36.9   -36.89  -37.053 -36.658 -35.963 -35.184 -34.462\n",
            " -33.784 -32.972 -32.546 -32.428 -32.292 -32.169 -31.998 -31.824 -31.624\n",
            " -31.433 -31.312 -31.107 -31.147 -31.044 -31.14  -31.417 -31.882 -32.145\n",
            " -32.487 -32.537 -32.334 -32.042 -31.892 -31.861 -31.991 -31.883 -31.89\n",
            " -31.941 -31.889 -31.724 -31.412 -31.786 -31.561 -31.732 -31.553 -31.862\n",
            " -32.389 -32.318 -32.19  -32.224 -32.368 -32.306 -32.305 -32.65  -33.093\n",
            " -33.263 -33.199 -33.27  -33.237 -33.221 -33.011 -32.844 -32.981 -32.784\n",
            " -32.607 -32.87  -33.196 -33.524 -33.956 -33.944 -33.71  -33.621 -33.552\n",
            " -33.828 -34.11  -34.38 ]\n",
            "date: ['2012-07-13T22:33:06.019200000' '2012-07-23T22:54:59.990400000'\n",
            " '2012-08-02T22:55:52.003200000' '2012-08-12T23:08:59.971200000'\n",
            " '2012-08-22T23:29:01.968000000' '2012-09-01T23:17:38.976000000'\n",
            " '2012-09-12T02:59:18.960000000' '2012-09-21T23:18:37.036800000'\n",
            " '2012-10-02T03:00:17.971200000' '2012-10-11T23:13:27.984000000'\n",
            " '2012-10-22T02:50:32.006400000' '2012-10-31T23:36:39.974400000'\n",
            " '2012-11-11T02:40:46.041600000' '2012-11-20T23:08:29.990400000'\n",
            " '2012-12-01T02:47:51.993600000' '2012-12-10T23:23:16.972800000'\n",
            " '2012-12-21T02:58:48.979200000' '2012-12-30T23:07:23.030400000'\n",
            " '2013-01-10T02:56:43.008000000' '2013-01-19T23:24:26.956800000'\n",
            " '2013-01-30T02:43:53.011200000' '2013-02-08T23:15:27.043200000'\n",
            " '2013-02-19T01:12:50.976000000' '2013-02-28T23:07:13.008000000'\n",
            " '2013-03-11T02:43:30.979200000' '2013-03-20T23:17:22.992000000'\n",
            " '2013-03-31T01:50:38.025600000' '2013-04-09T23:19:07.968000000'\n",
            " '2013-04-20T02:53:29.990400000' '2013-04-29T23:28:33.024000000'\n",
            " '2013-05-10T02:50:18.009600000' '2013-05-19T23:21:05.990400000'\n",
            " '2013-05-30T02:50:30.969600000' '2013-06-08T23:32:49.027200000'\n",
            " '2013-06-19T03:42:51.004800000' '2013-06-28T23:32:16.022400000'\n",
            " '2013-07-09T03:28:30.979199999' '2013-07-18T23:33:57.974400000'\n",
            " '2013-07-29T03:15:42.019200000' '2013-08-07T23:25:02.035200000'\n",
            " '2013-08-18T01:47:44.966400000' '2013-08-28T03:02:59.020800000'\n",
            " '2013-09-07T03:03:51.984000000' '2013-09-16T23:32:07.987200000'\n",
            " '2013-09-27T04:08:27.974400000' '2013-10-06T23:25:39.964800000'\n",
            " '2013-10-17T02:55:50.995200000' '2013-10-27T03:45:47.001600000'\n",
            " '2013-11-06T01:14:52.022400000' '2013-11-16T03:29:54.009600000'\n",
            " '2013-11-26T03:03:56.995200000' '2013-12-05T23:33:59.011200000'\n",
            " '2013-12-16T02:58:01.977600000' '2013-12-25T23:22:43.017600000'\n",
            " '2014-01-05T02:52:06.009600000' '2014-01-14T23:41:18.009600000'\n",
            " '2014-01-25T03:00:43.977600000' '2014-02-03T23:29:13.977600000'\n",
            " '2014-02-14T02:50:11.961600000' '2014-02-23T23:03:21.974400000'\n",
            " '2014-03-06T02:58:03.964800000' '2014-03-15T23:10:28.012800000'\n",
            " '2014-03-26T02:51:22.032000000' '2014-04-04T23:25:58.972800000'\n",
            " '2014-04-15T03:00:45.964800000' '2014-04-24T23:24:40.003200000'\n",
            " '2014-05-05T02:56:22.012800000' '2014-05-15T00:10:06.009600000'\n",
            " '2014-05-25T03:02:43.036800000' '2014-06-03T23:34:53.961600000'\n",
            " '2014-06-14T03:01:23.980800000' '2014-06-23T23:24:31.968000000'\n",
            " '2014-07-04T03:08:30.019200000' '2014-07-13T23:47:43.008000000'\n",
            " '2014-07-24T03:02:33.014400000']\n"
          ]
        }
      ],
      "source": [
        "# print out the values of the latitude, longitude, and date variables\n",
        "print(f\"latitude: {lat}\")\n",
        "print(f\"longitude: {lon}\")\n",
        "print(f\"date: {date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3vQ4Kgqv-L6"
      },
      "source": [
        "#### (e.)\n",
        "- prove to yourself that each of the data structures is a NumPy array\n",
        "- show me one example of this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNcMlzVPv-L6",
        "outputId": "cc136f7e-3ddc-4cc6-f227-d83cf0b0c450",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of S: <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# prove to yourself that each of the data structures is a NumPy array\n",
        "# show me one example of this!\n",
        "# print type of S\n",
        "print(f\"type of S: {type(S)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUQr0taYv-L7"
      },
      "source": [
        "#### (f.)\n",
        "- evaluate and print out the shape of S, T, P, lat, lon, and levels  \n",
        "- *Note*: levels is UNITLESS\n",
        "- from your above evaluations, please identify and name what the two dimensions of S, T, and P are (and tell me in a comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9fw3OZPv-L7",
        "outputId": "ed3d7406-8bfc-451b-c1df-5dc546a96bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S shape: (78, 75)\n",
            "T shape: (78, 75)\n",
            "P shape: (78, 75)\n",
            "lat shape: (75,)\n",
            "lon shape: (75,)\n",
            "levels shape: (78,)\n"
          ]
        }
      ],
      "source": [
        "# evaluate and print out the shape of S, T, P, lat, lon, and levels\n",
        "# Note: levels is UNITLESS\n",
        "print(f\"S shape: {S.shape}\")\n",
        "print(f\"T shape: {T.shape}\")\n",
        "print(f\"P shape: {P.shape}\")\n",
        "print(f\"lat shape: {lat.shape}\")\n",
        "print(f\"lon shape: {lon.shape}\")\n",
        "print(f\"levels shape: {levels.shape}\")\n",
        "\n",
        "# from your above evaluations, please identify and name what the two dimensions of S, T, and P are (and tell me in a comment)\n",
        "# Since the float drifts with the ocean currents, each vertical profile of temperature, salinity, and pressure is associated with a given date/time and a given location - i.e., the latitude/longitude point\n",
        "# the two dimensions of S, T, and P are:\n",
        "# 1. date/time\n",
        "# 2. location - i.e., the latitude/longitude point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XviIm0Bwv-L7"
      },
      "source": [
        "#### (g.) evaluate/calculate and print:\n",
        "- are there NaNs?\n",
        "- if so, how many there are in each array (S, T, and P)\n",
        "- how this compares to the total # of elements in each array (S, T, and P)\n",
        "- the percent of NaNs in each array (S, T, and P) (relative to the total # of data points in each array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll3winMFv-L7",
        "outputId": "54109360-762a-41ba-8caf-c52ac99344bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaNs in S: True\n",
            "NaNs in T: True\n",
            "NaNs in P: True\n",
            "NaNs in S: 101\n",
            "NaNs in T: 101\n",
            "NaNs in P: 91\n",
            "Total elements in S: 5850\n",
            "Total elements in T: 5850\n",
            "Total elements in P: 5850\n",
            "Percent of NaNs in S: 0.017264957264957265\n",
            "Percent of NaNs in T: 0.017264957264957265\n",
            "Percent of NaNs in P: 0.015555555555555555\n"
          ]
        }
      ],
      "source": [
        "# evaluate/calculate and print:\n",
        "# are there NaNs?\n",
        "print(f\"NaNs in S: {np.isnan(S).any()}\")\n",
        "print(f\"NaNs in T: {np.isnan(T).any()}\")\n",
        "print(f\"NaNs in P: {np.isnan(P).any()}\")\n",
        "\n",
        "# if so, how many there are in each array (S, T, and P)\n",
        "print(f\"NaNs in S: {np.isnan(S).sum()}\")\n",
        "print(f\"NaNs in T: {np.isnan(T).sum()}\")\n",
        "print(f\"NaNs in P: {np.isnan(P).sum()}\")\n",
        "\n",
        "# how this compares to the total # of elements in each array (S, T, and P)\n",
        "print(f\"Total elements in S: {S.size}\")\n",
        "print(f\"Total elements in T: {T.size}\")\n",
        "print(f\"Total elements in P: {P.size}\")\n",
        "\n",
        "# the percent of NaNs in each array (S, T, and P) (relative to the total # of data points in each array)\n",
        "print(f\"Percent of NaNs in S: {np.isnan(S).sum()/S.size}\")\n",
        "print(f\"Percent of NaNs in T: {np.isnan(T).sum()/T.size}\")\n",
        "print(f\"Percent of NaNs in P: {np.isnan(P).sum()/P.size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLi99QePv-L7"
      },
      "source": [
        "#### (h.) calculate and print the maximum and minimum value of each array (S, T, and P), over ALL depths and ALL latitude/longitude points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcQdOCLlv-L7",
        "outputId": "0eb5c431-9dd6-4cc7-dad5-087ed49b59f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max S: 36.2849006652832\n",
            "Min S: 34.89790344238281\n"
          ]
        }
      ],
      "source": [
        "# calculate and print the maximum and minimum value of each array (S, T, and P), over ALL depths and ALL latitude/longitude points\n",
        "print(f\"Max S: {np.nanmax(S)}\")\n",
        "print(f\"Min S: {np.nanmin(S)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8E2U7i1v-L8",
        "outputId": "4a12dc4a-6db7-4272-972e-0a100e39b43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max T: 24.364999771118164\n",
            "Min T: 3.5320000648498535\n"
          ]
        }
      ],
      "source": [
        "print(f\"Max T: {np.nanmax(T)}\")\n",
        "print(f\"Min T: {np.nanmin(T)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJG_ZEuiv-L8",
        "outputId": "e158c284-e652-4fde-d075-6be95350cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max P: 2001.5\n",
            "Min P: 0.30000001192092896\n"
          ]
        }
      ],
      "source": [
        "print(f\"Max P: {np.nanmax(P)}\")\n",
        "print(f\"Min P: {np.nanmin(P)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84K8Ulhv-L8"
      },
      "source": [
        "#### (i.) \n",
        "*Keep in mind that you've shown that S, T, and P are two dimensional variables and what each dimension is. Now:*\n",
        "- create a line plot for each of these variables, initially just for the *first* latitude/longitude point in the dataset\n",
        "   - we want the vertical axis of this plot to be the vertical levels in the ocean \n",
        "      - please reverse the vertical axis, so it reads, from top to bottom, 0 to 77, as the \"higher\" values of the 'level' variable are actually points *deeper* in the ocean\n",
        "   - and the horizontal axis of this plot to be the quantity of interest\n",
        "   - you must label both axises (including units - see start of assignment), and add a title "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWOAu2rv-L8"
      },
      "outputs": [],
      "source": [
        "# Keep in mind that you've shown that S, T, and P are two dimensional variables and what each dimension is. Now:\n",
        "# create a line plot for each of these variables, initially just for the first latitude/longitude point in the dataset\n",
        "## we want the vertical axis of this plot to be the vertical levels in the ocean\n",
        "### please reverse the vertical axis, so it reads, from top to bottom, 0 to 77, as the \"higher\" values of the 'level' variable are actually points deeper in the ocean\n",
        "## and the horizontal axis of this plot to be the quantity of interest\n",
        "## you must label both axises (including units - see start of assignment), and add a title\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(S[:, 0], levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Salinity (psu)')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Salinity vs Levels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SU9qKZmv-L8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(T[:, 0], levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Temperature (deg C)')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Temperature vs Levels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_OkVMfSv-L8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(P[:, 0], levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Pressure (dbar)')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Pressure vs Levels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQjNqRm8v-L8"
      },
      "source": [
        "#### (j.) \n",
        "- now, let's take your plotting up a notch!\n",
        "- you'll still create THREE plots in total, one for each S, P, and T\n",
        "  - with appropriate titles, axis labels, etc.\n",
        "<br><br>\n",
        "- with vertical axis being levels (reversed) and horizontal axis being values of the given variable, like you did in (i.)\n",
        "<br><br>\n",
        "- but now you'll have many profiles on the same plot -- data of that variable collected at each and every lat/lon gridpoint contained in the dataset\n",
        "   - no legends necessary\n",
        "- there are multiple ways to do this\n",
        "   - the simplest involves being clever in your line(s) of code that does the plotting \n",
        "<br><br>\n",
        "- *option if you want to challenge yourself - ungraded*\n",
        "  - instead, plot all three of the above-described plots on a single plot that consists of 3 subplots "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB6YgSGiv-L8"
      },
      "outputs": [],
      "source": [
        "# now, let's take your plotting up a notch!\n",
        "# you'll still create THREE plots in total, one for each S, P, and T with appropriate titles, axis labels, etc.\n",
        "\n",
        "# with vertical axis being levels (reversed) and horizontal axis being values of the given variable, like you did in (i.)\n",
        "\n",
        "# but now you'll have many profiles on the same plot -- data of that variable collected at each and every lat/lon gridpoint contained in the dataset no legends necessary\n",
        "\n",
        "# there are multiple ways to do this: the simplest involves being clever in your line(s) of code that does the plotting\n",
        "\n",
        "# option if you want to challenge yourself - ungraded : instead, plot all three of the above-described plots on a single plot that consists of 3 subplots\n",
        "graph, (plot1, plot2, plot3) =plt.subplots(nrows=1, ncols=3, figsize=(30,10))\n",
        "plot1.plot(S, levels)\n",
        "plot1.invert_yaxis()\n",
        "plot1.set_xlabel('Salinity (psu)')\n",
        "plot1.set_ylabel('Levels')\n",
        "plot1.set_title('Salinity vs Levels')\n",
        "\n",
        "plot2.plot(T, levels)\n",
        "plot2.invert_yaxis()\n",
        "plot2.set_xlabel('Temperature (deg C)')\n",
        "plot2.set_ylabel('Levels')\n",
        "plot2.set_title('Temperature vs Levels')\n",
        "\n",
        "plot3.plot(P, levels)\n",
        "plot3.invert_yaxis()\n",
        "plot3.set_xlabel('Pressure (dbar)')\n",
        "plot3.set_ylabel('Levels')\n",
        "plot3.set_title('Pressure vs Levels')\n",
        "\n",
        "# tight layout\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcoZLIKDv-L8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIY0Omcv-L8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCIaQu3v-L8"
      },
      "source": [
        "#### (k.)\n",
        "Now, let's interpret the plots you produced in (j.)\n",
        "<br><br>\n",
        "- (a.) For each variable (so, for each plot) - **visually**, at which levels is there the greatest spread of values w/ location? \n",
        "<br><br>\n",
        "- (b.) What formal, simple statisical metric could we use to quantiatively evaluate spread?\n",
        "<br><br>\n",
        "- (c.) Calculate spread at each height for each variable using this metric, and create a plot for each - so, a vertical profile of spread (x-axis) with depth (y-axis) for each variable - there'll be a SINGLE line on each plot, yes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf1M2Hbhv-L9"
      },
      "outputs": [],
      "source": [
        "# (a.) For each variable (so, for each plot) - visually, at which levels is there the greatest spread of values w/ location?\n",
        "# Solution:\n",
        "# Visually, for variable S, the greatest spread of values is at level 0, which is the surface level.\n",
        "# Visually, for variable T, the greatest spread of values is at level 0, which is the surface level.\n",
        "# Visually, for variable P, the greatest spread of values is quite difficult to determine, since \n",
        "####           the Pressure values range from 0 to 2000 dbar, and there are many different levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCIAUqRvv-L9"
      },
      "outputs": [],
      "source": [
        "# (b.) What formal, simple statisical metric could we use to quantiatively evaluate spread?\n",
        "# solution: Interquartile range (IQR) could be used to quantify spread.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtRMbNclv-L9"
      },
      "outputs": [],
      "source": [
        "# (c.) Calculate spread at each height for each variable using this metric, and create a plot for each - so, a vertical profile of spread (x-axis) with depth (y-axis) for each variable - there'll be a SINGLE line on each plot, yes?\n",
        "# For S, get the IQR of the S array across axis 1 (the vertical axis), ignoring NaNs., \n",
        "# and then plot the IQR against the levels array.\n",
        "S_IQR = np.nanpercentile(S, 75, axis=1) - np.nanpercentile(S, 25, axis=1)\n",
        "print(f\"S_IQR Shape is {S_IQR.shape}\")\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(S_IQR, levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Spread')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Spread vs Levels (S)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju01Bqnyv-L9"
      },
      "outputs": [],
      "source": [
        "T_IQR = np.nanpercentile(T, 75, axis=1) - np.nanpercentile(T, 25, axis=1)\n",
        "print(f\"T_IQR Shape is {T_IQR.shape}\")\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(T_IQR, levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Spread')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Spread vs Levels (T)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "P_IQR = np.nanpercentile(P, 75, axis=1) - np.nanpercentile(P, 25, axis=1)\n",
        "print(f\"P_IQR Shape is {P_IQR.shape}\")\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(P_IQR, levels)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Spread')\n",
        "plt.ylabel('Levels')\n",
        "plt.title('Spread vs Levels (P)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCQyGe5Pv-L9"
      },
      "source": [
        "#### (l.)\n",
        "- time to plot the locations where all these profiles were collected!\n",
        "  - including axis labels!\n",
        "- you haven't formally learned how to create a map yet, so simply use a scatter plot\n",
        "<br><br>\n",
        "- *options if you'd like to challenge yourself - ungraded*\n",
        "  - know how to create maps?  plot this data on a map instead of scatter plot!\n",
        "  - color-code the points by date!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRsds1Frv-L9"
      },
      "outputs": [],
      "source": [
        "# time to plot the locations where all these profiles were collected!, you haven't formally learned how to create a map yet, so simply use a scatter plot\n",
        "# x-axis is latitude, y-axis is longitude\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(lat, lon)\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.title('Latitude vs Longitude')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert each value of date numpy array to a datetime object\n",
        "import datetime\n",
        "date = date.astype(datetime.datetime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a pandas dataframe with columns as lat, lon, date \n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'lat': lat, 'lon': lon, 'date': date})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# options if you'd like to challenge yourself - ungraded\n",
        "# know how to create maps? plot this data on a map instead of scatter plot!\n",
        "# color-code the points by date!\n",
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "geometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n",
        "gdf = GeoDataFrame(df, geometry=geometry)   \n",
        "\n",
        "#this is a simple map that goes with geopandas\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "gdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=15);\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mehta_Assignment3_Students.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
